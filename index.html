<!doctype html>
<html>

<head>
    <meta charset="utf-8">

    <title>Hi there! I'm å‘¨å²±ğŸ‘‹</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


    <!-- set page stylesheet -->
    <link rel="stylesheet" href="/css/shared.css" type="text/css">
    <link rel="stylesheet" href="/css/navbar.css" type="text/css">

</head>

<body>

    <div class="container_bg navbar">
        <h2>
            <a href="/index.html">HOME</a>&nbsp;&nbsp;<span class="delimiter">&middot;</span>&nbsp;&nbsp;
            <a href="/project.html">PROJECT</a>&nbsp;&nbsp;<span
                class="delimiter">&middot;</span>&nbsp;
            <a href="/record.html">RECORD</a>
            <!-- <a href="/open_source.html">Open Source</a>&nbsp;&nbsp;&middot;&nbsp; -->
            <!-- <a href="/awards.html">Awards</a> -->
        </h2>
    </div>

    <div class="container">
        <div id="group" align="center">
            <h1>Hi there! ğŸ‘‹ Welcome to my homepage!</h1>
        </div>
        <div id="sidebar">
            <img src="/img/avatar.jpg" vspace="30 px" width="250 px" id="me" itemprop="photo"></img>
        </div>

        <div id="bio">

            <br>

            <h1>
                <span itemprop="name">Dai Zhou <font size="6">å‘¨å²±</font> </span>
            </h1>

            <p style="line-height:23px;">
                Undergraduate
                <br>
                <br>
                Robotics Engineering Major
                <br>
                Xidian University
                <br>
                Xi'an, China 710071
                <br>
                <br>
                <br>
                Email: 2633127336@qq.com

                <br>
            </p>
        </div>
    </div>

    <div class="container">
        <h2>Recuit</h2>
        <p>
            
        </p>
        <p class="chinese">
            
        </p>


        <details class="chinese">
            <summary><strong style="font-family: sans-serif">ç‚¹å‡»æŸ¥çœ‹ç”³è¯·è¦æ±‚</strong></summary>
            <p class="bg_blue">
                <strong style="color: blue; font-family: sans-serif">æœ¬ç§‘å®ä¹ ç”Ÿç”³è¯·ï¼š</strong>
            <ul>
                <li>åŸºæœ¬è¦æ±‚ï¼šä¸Šæµ·äº¤é€šå¤§å­¦æœ¬ç§‘å¤§ä¸€ï¼Œå¤§äºŒï¼Œå¤§ä¸‰å­¦ç”Ÿ</li>
                <li>é™„ä»¶ææ–™ï¼šç®€å†</li>
                <li>é‚®ä»¶é¢˜ç›®ï¼šæœ¬ç§‘å®ä¹ -å§“å-ä¸“ä¸š</li>
            </ul>
            æ›´å¤šå…³äºæœ¬ç§‘å®ä¹ çš„ä¿¡æ¯è¯·è§<a href="/recruit/2024.html" class="first">è¯¦æƒ…</a>ã€‚
            </p>
            <p class="bg_blue">
                <strong style="color: blue; font-family: sans-serif">ç¡•å£«ã€åšå£«ç”³è¯·</strong>
            <ul>
                <li>åŸºæœ¬è¦æ±‚ï¼šç›´åšéœ€è·å¾—å½“å¹´æ‰€åœ¨å­¦æ ¡çš„æ¨å…åé¢ï¼Œæ™®åšéœ€å…¥å­¦æ—¶è·å¾—ç¡•å£«å­¦ä½</li>
                <li>é™„ä»¶ææ–™ï¼šç®€å†</li>
                <li>é‚®ä»¶é¢˜ç›®ï¼šç¡•å£«ç”³è¯·-å§“å-å­¦æ ¡ï¼Œç›´åšç”³è¯·-å§“å-å­¦æ ¡ï¼Œæ™®åšç”³è¯·-å§“å-åœ¨è¯»å­¦æ ¡</li>
            </ul>
            </p>

            <p class="bg_red">
                <strong style="color: red; font-family: sans-serif">åšå£«åç”³è¯·</strong>
            <ul>
                <li>åŸºæœ¬è¦æ±‚ï¼šå…¥èŒæ—¶è·å¾—åšå£«å­¦ä½</li>
                <li>é™„ä»¶ææ–™ï¼šç®€å†</li>
                <li>é‚®ä»¶é¢˜ç›®ï¼šåšå£«åç”³è¯·-å§“å-åšå£«æ¯•ä¸šå­¦æ ¡</li>
            </ul>
            </p>

            <p class="bg_red">
                <strong style="color: red; font-family: sans-serif">Research APï¼ˆç ”ç©¶åŠ©ç†æ•™æˆ/ç ”ç©¶å‘˜ï¼‰ç”³è¯·</strong>
            <ul>
                <li>åŸºæœ¬è¦æ±‚ï¼šè·å¾—åšå£«å­¦ä½ï¼Œç¬¬ä¸€ä½œè€…ï¼ˆå«å…±ä¸€ï¼‰èº«ä»½å‘è¡¨5ç¯‡åŠä»¥ä¸ŠAIå’Œæœºå™¨äººé¢†åŸŸé«˜è´¨é‡è®ºæ–‡</li>
                <li>æ”¿ç­–ä¸å¾…é‡ï¼šä¸­çº§èŒç§°ï¼Œé€šè¿‡ç”³è¯·å¯è·å¾—ç¡•å£«ç”Ÿå¯¼å¸ˆèµ„æ ¼ï¼›ç¬¬ä¸€ä¸ªè˜è¯·ä¸ºä¸‰å¹´ï¼Œé€šè¿‡è€ƒæ ¸å¯åœ¨ä¸Šæµ·äº¤é€šå¤§å­¦æ™‹å‡ä¸ºå‰¯æ•™æˆï¼Œå·¥èµ„å¾…é‡ä¼˜åš</li>
                <li>é™„ä»¶ææ–™ï¼šç®€å†</li>
                <li>é‚®ä»¶é¢˜ç›®ï¼šç ”ç©¶åŠ©ç†æ•™æˆ-å§“å-åšå£«æ¯•ä¸šå­¦æ ¡</li>
            </ul>
            </p>
        </details>




    </div>

    <div class="container">
        <h2>Short Bio</h2>
        <p>
            <b>Cewu Lu</b>, Professor at Shanghai Jiao Tong University, Distinguished Professor of the Changjiang
            Scholars Program (é•¿æ±Ÿå­¦è€…ç‰¹è˜æ•™æˆ), and recipient of the Scientific Exploration Award. In 2016, he was selected as
            a high-level overseas young talent, and in 2018, he was named one of the 35 Innovators Under 35 in China by
            MIT Technology Review (MIT TR35). In 2019, he received the Qiushi Outstanding Young Scholar Award,
            and in
            2020, he was the third contributor to the Shanghai Science and Technology Progress Special Award. In 2022,
            he received the Ministry of Education Youth Science Award and was recognized for one of the six best papers
            at IROS (out of 3579 submissions). In 2023, he was nominated for the Best
            System Paper Award at the Robotics: Science and Systems (RSS) conference (one of four nominations) and
            received the Scientific Exploration Award (the only recipient in the field of embodied intelligence). 
            In 2025, he was nomiated for Best Student Paper Award at RSS conference (one of three) and won Best Paper Award on Human-Robot Interaction at ICRA 2025 conference.
            <br><br>
            Before he joined SJTU, he was a research fellow at Stanford University working under Prof. Fei-Fei Li and
            Prof. Leonidas J. Guibas. He was a Research Assistant Professor at Hong Kong University of Science and
            Technology with Prof. Chi Keung Tang. He got the his PhD degree from The Chinese Univeristy of Hong Kong,
            supervised by Prof. Jiaya Jia.
            <br><br>
            As a corresponding author or first author, he has published more than 100 papers in high-impact journals and
            conferences, including <b>Nature</b>, <b>Nature Machine Intelligence</b>, and TPAMI. He serves as a reviewer
            for Science,
            Nature sub-journals, Cell sub-journals, and as area chair for NeurIPS, CVPR, ICCV, ECCV, IROS, and ICRA. His
            research interests include embodied intelligence and computer vision.
        </p>

        <hr width="80%">

        <p class="chinese">
            å¢ç­–å¾è€å¸ˆæ›¾åœ¨æ–¯å¦ç¦å¤§å­¦äººå·¥æ™ºèƒ½å®éªŒå®¤ï¼ˆå¯¼å¸ˆï¼šæé£é£ï¼ŒLeo Guibasï¼‰ä»äº‹åšå£«åç ”ç©¶ã€‚è¿‘å¹´æ¥è·é•¿æ±Ÿå­¦è€…ç‰¹è˜æ•™æˆï¼Œå›½å®¶é’å¹´åƒäººè®¡åˆ’ï¼Œã€Šéº»çœç†å·¥ç§‘æŠ€è¯„è®ºã€‹35ä½35å²ä»¥ä¸‹ç§‘æŠ€ç²¾è‹±ï¼ˆMIT
            TR35ï¼‰ï¼Œç§‘å­¦æ¢ç´¢å¥–ï¼ˆå…¨å›½å…·èº«æ™ºèƒ½æ–¹å‘å”¯ä¸€ï¼‰ï¼Œæ±‚æ˜¯æ°å‡ºé’å¹´å­¦è€…ï¼Œæ•™è‚²éƒ¨é’å¹´ç§‘å­¦å¥–ï¼Œç§‘å­¦ä¸­å›½äººæ°å‡ºé’å¹´ç§‘å­¦å®¶ï¼Œå¤šå¹´è·å¾—çˆ±æ€å”¯å°”Elsevierä¸­å›½é«˜è¢«å¼•å­¦è€…ç­‰å¥–é¡¹è£èª‰ã€‚å›½é™…æœºå™¨äººé¡¶ä¼šIROSã€ICRAæœ€ä½³è®ºæ–‡è·å¾—è€…ï¼Œä¸¤æ¬¡è·å¾—å›½é™…æœºå™¨äººé¡¶ä¼šRSSæœ€ä½³ç³»ç»Ÿ/å­¦ç”Ÿè®ºæ–‡æåã€‚æ‹…ä»»ã€ŠNatureã€‹ã€ŠScienceã€‹å®¡ç¨¿äººï¼ŒCVPRã€ICCVã€ECCVã€NeurIPSã€AAAIç­‰äººå·¥æ™ºèƒ½é¡¶ä¼šçš„é¢†åŸŸä¸»å¸­ã€‚
            ç´¯è®¡ä»¥ç¬¬ä¸€æˆ–é€šè®¯ä½œè€…å‘è¡¨CCF-Aç±»æœŸåˆŠã€ä¼šè®®è®ºæ–‡100ä½™ç¯‡ï¼Œæ€»å¼•ç”¨é‡è¶…è¿‡2ä¸‡ï¼ŒåŒ…æ‹¬ã€ŠNatureã€‹æ­£åˆŠï¼Œã€ŠNatureã€‹æœºå™¨æ™ºèƒ½å­åˆŠï¼ˆä¸Šæµ·äº¤å¤§ç¬¬ä¸€ç¯‡ï¼‰ï¼ŒTPAMIï¼ŒIJCVï¼ŒT-ROï¼ŒIJRRï¼ˆæœºå™¨äººé¢†åŸŸTOPæœŸåˆŠï¼‰ç­‰ã€‚
        </p>

    </div>

    <div class="container">
        <h2>Our Task and Vision</h2>
        <p>
            The development of <b>Robots for general-purpose</b> has long been a shared dream of humanity, as their
            realization
            would significantly enhance productivityâ€”by, for instance, performing tasks typically undertaken by nurses
            or cleaning staffâ€”and improve the quality of life through applications such as domestic service robots. A
            general-purpose robot must be capable of executing a wide range of tasks in diverse and open-ended
            environments, posing a formidable challenge in the field of artificial intelligence. The crux of this
            challenge lies in enabling robots to acquire human behavioral capabilities.

            Building upon a strong foundation in the visual understanding of human behavior, we aim to explore a novel
            approach: empowering robots to learn comprehensive, general-purpose behaviors by observing and interpreting
            vast amounts of human activity in video form. Compared to the mainstream
            approach of guiding robotic behavior through large language models, our strategy offers several advantages
            in achieving generalizability:
        <ul>
            <li><b>Autonomous Learning:</b> Human behavior videos capture the full scope of tasks and their various
                possibilities, circumventing the limitations of manually defined tasks.</li><br>
            <li><b>Scalability:</b> The extensive accumulation of videos through the growth of the internet covers
                nearly
                every conceivable human task and operation.</li><br>
            <li><b>Automatic Expansion:</b> With large volumes of new videos uploaded daily, robots can automatically
                extend
                their capabilities to include new tasks and skills.</li>
        </ul>

        By implementing this innovative approach, we pave the way for a more universally applicable and effective
        solution to the challenge of creating general-purpose robots.
        </p>

        <!-- <hr width="80%"> -->
        <br>
        <div class="image_line">
            <img src="recruit/2024/æœºå™¨äººåˆ®èƒ¡å­.gif" width="180px">
            <img src="recruit/2024/æœºå™¨äººå è¡£æœ.gif" width="180px">
            <img src="recruit/2024/æœºå™¨äººå‰Šé»„ç“œ.gif" width="180px">
            <img src="recruit/2024/æœºå™¨äººæ”¶æ‹¾å®¶åŠ¡.gif" width="180px">
        </div>
        <br>

        <!-- <hr width="80%"> -->

        <p class="chinese">
            å®éªŒå®¤ç ”ç©¶æ–¹å‘ä¸ºé€šç”¨æœºå™¨äººï¼ˆå…·èº«æ™ºèƒ½ï¼‰ï¼Œæ˜¯å®ç°é€šç”¨äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚ä¸è¢«åŠ¨æ¥æ”¶ä¿¡æ¯ä¸åŒï¼Œå…·èº«æ™ºèƒ½å¼ºè°ƒæ™ºèƒ½ä½“ä¸»åŠ¨ä¸ç‰©ç†ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚äººç±»ä¹Ÿåœ¨æ–‡è‰ºä½œå“é‡Œæç»˜è¿‡å¾ˆå¤šæµªæ¼«çš„æœºå™¨äººæ•…äº‹ï¼Œæ¯”å¦‚ã€Šæœºå™¨ç®¡å®¶ã€‹ä¸­çš„å®‰å¾·é²ã€ã€Šæœºå™¨äººæ€»åŠ¨å‘˜ã€‹ä¸­çš„ç“¦åˆ©å’Œå¤å¨ƒã€ã€Šæ˜Ÿçƒå¤§æˆ˜ã€‹é‡Œçš„R2-D2å’ŒC-3POç­‰ç­‰ã€‚
            å…·èº«æ™ºèƒ½é©±åŠ¨çš„é€šç”¨æœºå™¨äººå°†æ˜¯äººå·¥æ™ºèƒ½çš„ç»ˆæçŠ¶æ€ï¼Œä¹Ÿæ˜¯ç›®å‰å›½é™…å­¦æœ¯å’Œäº§ä¸šå‰æ²¿ã€‚è‹±ä¼Ÿè¾¾CEOé»„ä»å‹‹åœ¨2023 world ITFå¤§ä¼šè¡¨ç¤ºï¼šThe next wave of AI, known as embodied
            AI, refers to intelligent systems that can understand, reason about, and interact with the physical
            worldã€‚å³ï¼Œäººå·¥æ™ºèƒ½çš„ä¸‹ä¸€æ³¢æµªæ½®è¢«ç§°ä¸ºå…·èº«æ™ºèƒ½ï¼Œå®ƒæŒ‡çš„æ˜¯èƒ½å¤Ÿç†è§£ã€æ¨ç†å’Œä¸ç‰©ç†ä¸–ç•Œäº’åŠ¨çš„æ™ºèƒ½ç³»ç»Ÿã€‚æœŸå¾…å¤§å®¶çš„åŠ å…¥å’Œæˆ‘ä»¬ä¸€èµ·æ¨åŠ¨æ¨åŠ¨å…·èº«æ™ºèƒ½çš„å˜é©ã€‚
        </p>

    </div>

    <!-- <div class="container">
        <h2>News</h2>
        <p>
            [2024] Six papers were accepted by ECCV 2024.
            <br>
            <br>
            [2024] Four papers were accepted by ICRA 2024.
            <br>
            <br>
            [2024] Five papers were accepted by CVPR 2024.
            <br>
            <br>
            [2022] One paper was accepted by <strong>Nature</strong> (One of two corresponding authors)
            <a href="https://www.nature.com/articles/s41586-022-04507-5">Link</a>.
            <br>
            <br>
            [2022] Ten papers were accepted by CVPR 2022.
            <br>
            <br>
            [2020] One paper was accepted by <strong>Nature Machine Intelligence</strong>
            <a href="https://www.nature.com/articles/s42256-020-0168-3">Link</a>.
            <br>
            <br>
            [2020] The concept of the General-Purpose Intelligent Agent
            (<a href="https://www.sciencedirect.com/science/article/pii/S2095809919309075">GIA</a>)
            has been published in <strong>Engineering</strong>.
            <br>
            <br>
            [2020] Seven papers were accepted by <strong>CVPR</strong> 2020
            <a href="/publications.html">Link</a>.
            <br>
            <br>
            [2020] Prof. Lu Cewu will serve as an <strong>Area Chair</strong> for CVPR 2020.
            <br>
            <br>
            [2020] <b><a href="http://hake-mvig.cn">
                    <span style="color: red" size="6px">H</span><span style="color: blue" size="6px">A</span><span
                        style="color: red" size="6px">KE</span></a></b>: Human Activity Knowledge Engine begins trial
            operation!
            <a href="http://hake-mvig.cn">Link</a>.
            <br>
            <br>
            [2019] VALSE 2019: Knowledge Driven Human Activity Understanding
            <a href="/research/VALSE2019.html">Talk</a>.
            <br>
            <br>
            [2018] PRCV 2018: Activity Understanding meets 3D Representation
            <a href="/research/PRCV2019.html">Talk</a>.
            <br>
            <br>
            [2017] AlphaPose is released, a pose estimation system based on our RMPE[ICCV'17], relatively outperforms
            Mask RCNN by 8.2% on COCO dataset pose task.
            <a href="/research/alphapose.html">AlphaPose</a>.
            <br>
            <br>
            <!-- [Year] We propose a new task of Action Adverb recognition.
            <a href="/research/adha/adha.html">ADHA</a>.
            <br> -->
    <!-- [2017] Prof. Lu Cewu is selected as MIT TR35 - "MIT Technology Review, 35 Innovators Under 35 (China)"
            <a href="http://www.mittrchina.com/news/1623">MIT Technology Review</a>. -->
    </p>
    </div> -->

</body>

</html>